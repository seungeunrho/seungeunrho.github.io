
<!DOCTYPE html>
<html lang="en">
<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.9.0/p5.js"></script><script src="assets/icon.js"></script>
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Learning To Swing Using Reinforcement Learning</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="../css/normalize.css">
  <link rel="stylesheet" href="../css/skeleton.css">

</head>
<body>

</head>
<body>

<div class="container">
    <div class="row"  style="margin-top: 4%">
      <div class="twelve columns">

        <h1>Learning To Swing Using Reinforcement Learning</h1>

        <p>In collaboration with <a href="http://www.theopanag.com/">Theo Panagiotopoulos</a></p>
        
        <video id="learning_to_swing" class="no-controls-video" autoplay loop muted preload="auto">
          <source src="../assets/videos/learning_to_swing.mp4" type="video/mp4">
        </video>

        <p>
        This project is a Final Project of <a href="https://www.cc.gatech.edu/~karenliu/Home.html">Dr. C. Karen Liu</a>'s Computer Animation class. <br><br>
        An off-the-shelf Off-policy Reinforcement Learning method <a href="https://spinningup.openai.com/en/latest/algorithms/sac.html">Soft-Actor-Critic</a> has been used to train body to build up the momentum on a pull up bar.<br>
        The control policy is conditioned on a <strong>state</strong> that consists of positions and velocities of all character joints.
        <strong>Policy</strong> produces a desired delta position for the following DOF: thigh, shin, toe, bicep, forearm, hand1&2, head.<br>
        Target positions are input to the PID controller which operates at a higher frequency (policy skips frames). <br>
        Since this an idealistic simulator setting, left and right body parts are <strong>"mirrored"</strong>, i.e. left thigh and right thigh are initilized in the same respective configurations and both get the same control values.<br>
        <strong>Rewards</strong> is continious and given as absolute velocity of pelvis around <strong>z</strong> axis.

        NeuralNet consists of 1 layer Fully Connected layer of 256 neurons.

        <p>
        <em>Initial policy (0 mins of training):</em><br>
        <iframe width="720" height="400" src="https://www.youtube.com/embed/ZrbnHCP0nVo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <br>
        <em>Initial policy (15 mins of training):</em><br>
        <iframe width="720" height="400" src="https://www.youtube.com/embed/dLxKjDKMoiI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

        <br>
        <em>Initial policy (20 mins of training):</em><br>
        <iframe width="720" height="400" src="https://www.youtube.com/embed/N34kc6pcHq8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


      </div>
    </div>
</div>

</body>

</html>
