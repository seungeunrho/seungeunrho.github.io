
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Reference Grounded Skill Discovery</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!--    <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">-->
<!--    <meta property="og:image:type" content="image/png">-->
<!--    <meta property="og:image:width" content="1296">-->
<!--    <meta property="og:image:height" content="840">-->
<!--    <meta property="og:type" content="website" />-->
<!--    <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/>-->
<!--    <meta property="og:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />-->
<!--    <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />-->

<!--    <meta name="twitter:card" content="summary_large_image" />-->
<!--    <meta name="twitter:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />-->
<!--    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />-->
<!--    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" />-->


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí´</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N0LVZ5JF66"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N0LVZ5JF66');
</script>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center" style="font-weight: bold;">
                Reference Grounded Skill Discovery
                </br>
                <small>
								ICLR 2026
                </small>
            </h1>
        </div>

        <div class="row">
            <p></br></p>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <a href="https://seungeunrho.github.io/">
                          Seungeun Rho
                </a>,&nbsp;
                <a href="https://aarontrinh02.github.io/">
                            Aaron Trinh
                        </a>,&nbsp;
                <a href="https://faculty.cc.gatech.edu/~danfei/">
                          Danfei Xu
                        </a>,&nbsp;
                <a href="https://faculty.cc.gatech.edu/~sha9/">
                          Sehoon Ha
                        </a>
                    </br>
                School of Interactive Computing </br>
                Georgia Institute of Technology
            </div>
        </div>

        <div class="row">
            <p></br></p>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2510.06203">
                            <image src="img/paper_image2.png" height="80px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://youtu.be/zBSH-k9GbV4">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="http://storage.googleapis.com/gresearch/refraw360/360_v2.zip" target="popup" onclick="window.open('http://storage.googleapis.com/gresearch/refraw360/360_v2.zip','popup','width=600,height=400')">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Dataset Pt. 1</strong></h4>-->
<!--                            </a>							-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip-->
<!--" target="popup" onclick="window.open('https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip-->
<!--','popup','width=600,height=400')">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Dataset Pt. 2</strong></h4>-->
<!--                            </a>							-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://github.com/google-research/multinerf">-->
<!--                            <image src="img/github.png" height="60px">-->
<!--                                <h4><strong>Code</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                    </ul>
                </div>
        </div>






        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    Motivation
                </h3>
                <img
                    src="img/rgsd_vs_metra2.png"
                    alt="RGSD vs METRA"
                    style="max-width: 60%; display: block; margin: 12px auto;"
                    />
                <p class="text-justify">
                Existing unsupervised skill discovery methods struggle to scale to high-DoF agents.
                Moreover, the learned skills are often unstructured and therefore lack semantic meaning.
                We propose <strong>Reference-Grounded Skill Discovery (RGSD)</strong>, which leverages a
                reference motion dataset to ground the online skill discovery process onto a semantically
                meaningful manifold. Through this reference grounding, RGSD learns diverse variations for
                a 69-DoF humanoid character around existing motions. For example, given a single backward-walking
                motion, RGSD can discover diverse turning behaviors while walking backward, leading to improved
                downstream task performance.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    How it works
                </h3>
                <img
                    src="img/method_overall.png"
                    alt="RGSD vs METRA"
                    style="max-width: 80%; display: block; margin: 12px auto;"
                    />
                <div class="col-md-8 col-md-offset-2">
                    <ul>
                        <li>
                            Using contrastive learning, we train an encoder q(ùëß|s) to map each motion to a directional vector on the unit hypersphere.
                        </li>
                        <li>
                            We then use the pre-trained encoder as a discriminator within DIAYN.
                        </li>
                        <li>
                            When the policy conditioned on a specific motion embedding ùëß, the DIAYN reward acts as an imitation reward.
                        </li>
                        <li>
                            When the policy conditioned on a different ùëß, the DIAYN reward encourages the discovery of novel yet relevant behaviors.
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    Video
                </h3>

                <!-- <p class="text-justify">
Using different prompts, LGSD enables the learning of skills with predefined semantic meanings. Here are the video of learned skills.
                </p> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h4 style="font-weight: bold;">
                     1) Learned skills from RGSD
                </h4>
                <video
                id="v2"
                style="height: auto; max-width: 80%;"
                controls
                playsinline
                >
                <source src="img/controllinv_diversity.mp4" type="video/mp4" />
                Your browser does not support the video tag.
                </video>
                <br>
                RGSD can control the degree of diversity at test time by varying the sampling distribution of the latent variable ùëß.
            </div>
            
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h4 style="font-weight: bold;">
                    2) When trained with unsegmented reference motions
                </h4>
                <video
                id="v2"
                style="height: auto; max-width: 80%;"
                controls
                playsinline
                >
                <source src="img/unsegmented_motion.mp4" type="video/mp4" />
                Your browser does not support the video tag.
                </video>

                <br>


                RGSD is applicable to unsegmented reference motions <br>
                (a single motion containing multiple skills)

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h4 style="font-weight: bold;">
                    3) Unconstrained motion generation
                </h4>
                <video
                id="v2"
                style="height: auto; max-width: 80%;"
                controls
                playsinline
                >
                <source src="img/unconstrained_generation.mp4" type="video/mp4" />
                Your browser does not support the video tag.
                </video>

                <br>
                Latent vectors are sampled uniformly from the unit hypersphere.

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <h4 style="font-weight: bold;">
                    4) Downstream task evaluation with learned skills
                </h4>
                <video
                id="v2"
                style="height: auto; max-width: 80%;"
                controls
                playsinline
                >
                <source src="img/downstream.mp4" type="video/mp4" />
                Your browser does not support the video tag.
                </video>



            </div>
        </div>




        <!-- <div class="row">
            <p></br></p>
        </div> -->


        <div class="row">
            <p></br></p>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <video
                id="v2"
                style="height: auto; max-width: 80%;"
                controls
                playsinline
                >
                <source src="img/RGSD_Video.mp4" type="video/mp4" />
                Your browser does not support the video tag.
                </video>
            </div>
        </div> -->



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://youtube.com/embed/zBSH-k9GbV4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{rho2025reference,
  title={Reference Grounded Skill Discovery},
  author={Rho, Seungeun and Trinh, Aaron and Xu, Danfei and Ha, Sehoon},
  journal={arXiv preprint arXiv:2510.06203},
  year={2025}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 style="font-weight: bold;">
                    Acknowledgements
                </h3>
                <p class="text-justify">
                Thanks to <a href="https://jhkim.me/">Jeonghwan Kim</a> for discussion.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
