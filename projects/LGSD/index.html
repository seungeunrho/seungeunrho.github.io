
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Language Guided Skill Discovery</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!--    <meta property="og:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg">-->
<!--    <meta property="og:image:type" content="image/png">-->
<!--    <meta property="og:image:width" content="1296">-->
<!--    <meta property="og:image:height" content="840">-->
<!--    <meta property="og:type" content="website" />-->
<!--    <meta property="og:url" content="https://jonbarron.info/mipnerf360/"/>-->
<!--    <meta property="og:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />-->
<!--    <meta property="og:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />-->

<!--    <meta name="twitter:card" content="summary_large_image" />-->
<!--    <meta name="twitter:title" content="Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields" />-->
<!--    <meta name="twitter:description" content="Though neural radiance fields (NeRF) have demonstrated impressive view synthesis results on objects and small bounded regions of space, they struggle on 'unbounded' scenes, where the camera may point in any direction and content may exist at any distance. In this setting, existing NeRF-like models often produce blurry or low-resolution renderings (due to the unbalanced detail and scale of nearby and distant objects), are slow to train, and may exhibit artifacts due to the inherent ambiguity of the task of reconstructing a large scene from a small set of images. We present an extension of mip-NeRF (a NeRF variant that addresses sampling and aliasing) that uses a non-linear scene parameterization, online distillation, and a novel distortion-based regularizer to overcome the challenges presented by unbounded scenes. Our model, which we dub 'mip-NeRF 360' as we target scenes in which the camera rotates 360 degrees around a point, reduces mean-squared error by 54% compared to mip-NeRF, and is able to produce realistic synthesized views and detailed depth maps for highly intricate, unbounded real-world scenes." />-->
<!--    <meta name="twitter:image" content="https://jonbarron.info/mipnerf360/img/gardenvase.jpg" />-->


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Language Guided Skill Discovery
                </br>
                <small>
								ICLR 2025
                </small>
            </h1>
        </div>

        <div class="row">
            <p></br></p>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://seungeunrho.github.io/">
                          Seungeun Rho
                        </a>
                        </br>Georgia Institute of Technology
                    </li>
                    <li>
                        <a href="https://lauramsmith.github.io/">
                            Laura Smith
                        </a>
                        </br>UC Berkeley
                    </li>
                    <li>
                        <a href="https://easypapersniper.github.io/">
                          Tianyu Li
                        </a>
                        </br>Georgia Institute of Technology
                    </li><br>
                    <li>
                        <a href="https://people.eecs.berkeley.edu/~svlevine/">
                          Sergey Levine
                        </a>
                        </br>UC Berkeley
                    </li>
                    <li>
                        <a href="https://xbpeng.github.io/">
                          Xue Bin Peng
                        </a>
                        </br>Simon Fraser University
                    </li>
                    <li>
                        <a href="https://faculty.cc.gatech.edu/~sha9/">
                          Sehoon Ha
                        </a>
                        </br>Georgia Institute of Technology
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2406.06615">
                            <image src="img/360_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://youtu.be/zBSH-k9GbV4">-->
<!--                            <image src="img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="http://storage.googleapis.com/gresearch/refraw360/360_v2.zip" target="popup" onclick="window.open('http://storage.googleapis.com/gresearch/refraw360/360_v2.zip','popup','width=600,height=400')">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Dataset Pt. 1</strong></h4>-->
<!--                            </a>							-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip-->
<!--" target="popup" onclick="window.open('https://storage.googleapis.com/gresearch/refraw360/360_extra_scenes.zip-->
<!--','popup','width=600,height=400')">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Dataset Pt. 2</strong></h4>-->
<!--                            </a>							-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://github.com/google-research/multinerf">-->
<!--                            <image src="img/github.png" height="60px">-->
<!--                                <h4><strong>Code</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                    </ul>
                </div>
        </div>






        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for unknown downstream tasks, obtaining a semantically diverse repertoire of skills is essential. While some approaches introduce a discriminator to distinguish skills and others aim to increase state coverage, no existing work directly addresses the "semantic diversity" of skills. We hypothesize that leveraging the semantic knowledge of large language models (LLMs) can lead us to improve semantic diversity of resulting behaviors. In this sense, we introduce Language Guided Skill Discovery (LGSD), a skill discovery framework that aims to directly maximize the semantic diversity between skills. LGSD takes user prompts as input and outputs a set of semantically distinctive skills. The prompts serve as a means to constrain the search space into a semantically desired subspace, and the generated LLM outputs guide the agent to visit semantically diverse states within the subspace. We demonstrate that LGSD enables legged robots to visit different user-intended areas on a plane by simply changing the prompt. Furthermore, we show that language guidance aids in discovering more diverse skills compared to five existing skill discovery methods in robot-arm manipulation environments. Lastly, LGSD provides a simple way of utilizing learned skills via natural language.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>

                <p class="text-justify">
Using different prompts, LGSD enables the learning of skills with predefined semantic meanings. Here are the video of learned skills.
                </p>




            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v2" width="100%" autoplay loop muted controls>
                  <source src="img/lgsd_edible.mp4" type="video/mp4" />
                </video>
            </div>
        </div>



        <div class="row">
            <p></br></p>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <video id="v1" width="80%" autoplay loop muted controls>
                  <source src="img/lgsd_manipulator.mp4" type="video/mp4" />
                </video>
            </div>
        </div>

        <div class="row">
            <p></br></p>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v2" width="100%" autoplay loop muted controls>
                  <source src="img/lgsd_ant4.mp4" type="video/mp4" />
                </video>
            </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Video-->
<!--                </h3>-->
<!--                <div class="text-center">-->
<!--                    <div style="position:relative;padding-top:56.25%;">-->
<!--                        <iframe src="https://youtube.com/embed/zBSH-k9GbV4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>-->
<!--                    </div>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{rho2024language,
  title={Language Guided Skill Discovery},
  author={Rho, Seungeun and Smith, Laura and Li, Tianyu and Levine, Sergey and Peng, Xue Bin and Ha, Sehoon},
  journal={arXiv preprint arXiv:2406.06615},
  year={2024}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                Thanks to Jeonghwan Kim for discussion.
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
