<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="icon" href="assets/icons/icon.png">
  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Maks Sorokin</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- <link rel="icon" type="image/png" href="assets/"> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1J19VDQYCK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-1J19VDQYCK');
  </script>
  <!-- Video playback speed and no controls -->
  <script defer src="js/video.js"></script>
</head>

<body>
  <div class="container">
    <!-- NAVBAR --------------------------------- -->
    <!-- <div class="navbar-spacer"></div>
    <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="index.html">About</a></li>
          <li class="navbar-item"><a class="navbar-link" href="/blog.html">Blog</a></li>
        </ul>
      </div>
    </nav> -->

    <!-- Personal Info  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row profile-row" style="margin-top: 4%">
      <div class="one columns"></div>
      <div class="three columns">
        <!-- Profile photo  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div style="text-align: center;">
          <img src="assets/img/photo.jpg" class="profile-photo" alt="profile photo"><br>
        </div>
        <!-- Name
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <h5 style="text-align: center;">Maks Sorokin</h5>
        <!-- Social links
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div id="social">
          <a href="https://twitter.com/initmaks"><img src="assets/icons/twitter.png" class="iico" /></a>
          <a rel="me" href="https://sigmoid.social/@maks"><img src="assets/icons/mastodon.png" class="iico" /></a>
          <a href="https://github.com/initmaks"><img src="assets/icons/github.png" class="iico" /></a>
          <a href="assets/pdfs/CV.pdf"><img src="assets/icons/file.png" class="iico" /></a>
          <img src="assets/icons/mail.png" style="cursor: pointer;" class="iico" id="iemail" title="click to reveal" />
        </div>
        <div id="demail"></div> <!-- will reveal email -->
      </div>

      <div class="eight columns" style="margin-top: 5%;">
        <p style="text-align:justify;">
          <!-- Self-introduction
          –––––––––––––––––––––––––––––––––––––––––––––––––– -->
          I'm a fourth-year Robotics Ph.D. student at Georgia Tech, advised by
          <a href="https://www.cc.gatech.edu/~sha9/" style="text-decoration: none;">Dr. Sehoon Ha</a>
          and
          <a href="https://ckllab.stanford.edu/" style="text-decoration: none;">Dr. C. Karen Liu</a>.
          I am interested in applications of vision-based robot learning in real-world robotics.
          Currently, I am working on outdoor navigation and environment interaction problems.
        </p>
        <!-- <strong>Competences:</strong>
        <a>python</a>,
        <a>pytorch</a>,
        <a>pybullet</a>,
        <a>iGibson</a>,
        <a>OpenCV</a>,
        <a>numpy</a>,
        <a>Tensorflow</a>,
        <a>C/C++</a>,
        <a>ROS</a>,
        <a>docker</a> -->

        <h5><u>News</u></h5>
        <ul>
          <li> <b>JUL'23</b> - Our work on Learning-oriented Robot Design was accepted at IROS 2023 <a
            href="https://learning-robot.github.io/">[project page]</a></li>
          <li> <b>MAR'23</b> - Check out our latest work on Design a Learning Robot <a
              href="https://learning-robot.github.io/">[project page]</a></li>
          <li> <b>APR'22</b> - Human Motion Control of Quadrupedal Robots accepted at RSS <a
              href="https://sites.google.com/view/humanconquad/">[project page]</a></li>
          <li> <b>SEP'21</b> - Excited to be joining <a href="https://x.company">X - the moonshot factory</a> ( <a
              href="https://everydayrobots.com">Everyday Robots</a> ) for PhD
            Residency!</li>
          <li> <b>SEP'21</b> - Check out our work on Learning Sidewalk Navigation <a
              href="./navigation.html">[project page]</a></li>
          <!-- <li> <b>MAY'21</b> - Awarded the fellowship by the Machine Learning Center at Georgia Tech. <a href="https://mlatgt.blog/2021/05/10/the-machine-learning-center-awards-inaugural-mlgt-fellows/">[link]</a> </li> -->
          <!-- <li> <b>FEB'21</b> Paper on Learning Human Search Behavior accepted at EUROGRAPHICS'2021! <a href="https://arxiv.org/pdf/2011.03618.pdf">[pdf]</a><a href="https://arxiv.org/abs/2011.03618">[arXiv]</a></li> -->
          <!-- <li> <b>DEC'20</b> Paper on Few-shot visual sensor meta-adaptation accepted at ICRA'2021! <a href="https://arxiv.org/pdf/2011.03609.pdf">[pdf]</a><a href="http://arxiv.org/abs/2011.03609">[arXiv]</a></li> -->
        </ul>


      </div>

    </div>
  </div>

  <!-- Companies
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br>
    <div class="row">
      <div class="one column hide-on-small">&nbsp;</div>
      <div class="five columns center">
        <a href="https://www.gatech.edu/">
          <img src="assets/icons/org/GeorgiaTechLogo.png" alt="Georgia Tech Logo" class="logo">
        </a>
        <p>
          PhD in Robotics<br>2020 - Present
        </p>
      </div>
      <div class="five columns center">
        <div class="row">
          <a href="https://everydayrobots.com">
            <img src="assets/icons/org/EverydayRobotsLogo.gif" alt="Everyday Robots Logo" class="logo">
          </a>
          <a href="https://x.company">
            <img src="assets/icons/org/X the moonshot factory logo.png" alt="Google X Logo" class="logo">
          </a>
        </div>
        <p>
          AI Residency<br>2021 - 2022
        </p>
      </div>
    </div>
  </div>
  
  <!-- Latest
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br>
    <h5><u>Latest Work</u></h5>


    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="learning-robot" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/learning_robot_morph.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">On Designing a Learning Robot: Improving Morphology for Enhanced Task Performance and
          Learning</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em><strong>Maks Sorokin</strong>, Chuyuan Fu, Jie Tan, C. Karen Liu, Yunfei Bai, Wenlong Lu, Sehoon Ha,
          Mohi Khansari</em>
        <br><em><span class="venue">International Conference on Intelligent Robots and Systems (IROS) 2023</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          We present a learning-oriented morphology optimization framework that accounts for the interplay between the
          robot's morphology, onboard perception abilities, and their interaction in different tasks.
          We find that morphologies optimized holistically improve the robot performance by 15-20% on
          various manipulation tasks, and require 25x less data to match human-expert made morphology performance.
        </p>
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://learning-robot.github.io/">[project page]</a>
        <a href="https://www.youtube.com/watch?v=w9B0COjGvfo">[video overview]</a>
        <a href="https://arxiv.org/pdf/2303.13390.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2303.13390">[arXiv]</a>
      </div>
    </div>

  </div>

  <!-- Publications
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <br>
    <div class="div_line"></div><br>
    <h5><u>Publications</u></h5>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="humanconquad" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/humanconquad.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em>Sunwoo Kim, <strong>Maks Sorokin</strong>, Jehee Lee, Sehoon Ha</em>
        <br><em><span class="venue">Proceedings of Robotics: Science and Systems (RSS) 2022</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          We propose a novel motion control system that allows a human user to operate various motor tasks seamlessly
          on a quadrupedal robot.
          Using our system, a user can execute a variety of motor tasks, including standing, sitting, tilting,
          manipulating, walking, and turning, on simulated and real quadrupeds.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://sites.google.com/view/humanconquad">[project page]</a>
        <a href="https://arxiv.org/pdf/2204.13336.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2204.13336">[arXiv]</a>
        <a href="https://www.youtube.com/watch?v=kz8hBG1CKMY">[video]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/behavior_representations.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Relax, it doesn't matter how you get there!</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em>Mehdi Azabou, Michael Mendelson, <strong>Maks Sorokin</strong>, Shantanu Thakoor, Nauman Ahad, Carolina
          Urzay, Eva L Dyer</em>
        <br><em><span class="venue">Neural Information Processing Systems (NeurIPS) 2023 - Spotlight</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          We introduce Bootstrap Across Multiple Scales (BAMS), a multi-scale self-supervised representation learning
          model for
          behavior analysis. We combine a pooling module that aggregates features extracted over encoders with different
          temporal
          receptive fields, and design latent objectives to bootstrap the representations in each respective
          space to encourage disentanglement across different timescales.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://multiscale-behavior.github.io/">[project page]</a>
        <a href="https://arxiv.org/pdf/2303.08811.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2303.08811">[arXiv]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/urban_navigation.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Learning to Navigate Sidewalks in Outdoor Environments</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em><strong>Maks Sorokin</strong>, Jie Tan, C. Karen Liu, Sehoon Ha</em>
        <br><em><span class="venue">IEEE Robotics and Automation Letters (RA-L) 2022</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          We design a system which enables zero-shot vision-based policy transfer to the real-world outdoor environments
          for sidewalk navigation task.
          Our approach is evaluated on a quadrupedal robot navigating sidewalks in the real world walking 3.2 kilometers
          with a limited number of human interventions.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="./navigation.html">[project page]</a>
        <a href="https://arxiv.org/pdf/2109.05603.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2109.05603">[arXiv]</a>
        <a href="https://www.youtube.com/watch?v=JsAZy3YETwQ">[video]</a>
        <a href="https://techxplore.com/news/2021-09-robot-efficiently-sidewalks-urban-environments.html">[TechXplore
          article]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/object_search_animation.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Learning Human Search Behavior from Egocentric View</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em><strong>Maks Sorokin</strong>, Wenhao Yu, Sehoon Ha, C. Karen Liu </em>
        <br><em><span class="venue">EUROGRAPHICS 2021</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          We train vision-based agent to perform object searching in photorealistic 3D scene.
          And propose a motion synthesis mechanism for head motion re-targeting.
          Using which we enable object searching behaviour with animated human character (PFNN/NSM).
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://arxiv.org/pdf/2011.03618.pdf">[pdf]</a>
        <a href="https://arxiv.org/abs/2011.03618">[arXiv]</a>
        <a href="https://www.youtube.com/watch?v=LvSHpmjt8pU">[video]</a>
        <a href="https://www.youtube.com/watch?v=NzsCT3a7rpY">[talk(20 min)]</a>
      </div>
    </div>


    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/sensor_height_adaptation.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">A Few Shot Adaptation of Visual Navigation Skills to New Observations using
          Meta-Learning</div>
        <!-- AUTHORS + VENUE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <br><em>Qian Luo, <strong>Maks Sorokin</strong>, Sehoon Ha</em>
        <br><em><span class="venue">The IEEE International Conference on Robotics and Automation (ICRA) 2021</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          We show how vision-based navigation agents can be trained to adapt to new sensor configurations with only
          three shots of experience.
          Rapid adaptation is achieved by introducing a bottleneck between perception and control networks, and through
          the perception component's meta-adaptation.
        </p>
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://arxiv.org/pdf/2011.03609.pdf">[pdf]</a>
        <a href="http://arxiv.org/abs/2011.03609">[arXiv]</a>
      </div>
    </div>

  </div>

  <!-- Projects
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <br>
    <div class="div_line"></div><br>
    <h5><u>Projects</u></h5>

    <!-- PROJECT ENTRY ###
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION
      –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <img src="assets/img/sim2sim.png">
        </div>
      </div>
      <div class="eight columns">
        <!-- TITLE
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Real2Sim Image adaptation</div>
        <br><em><span class="venue">2019</span></em>
        <!-- DESCRIPTION
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          Image domain adaptation through the conversion of images with randomized textures (or real images) to a
          canonical image representation.
          Replication of a <a href="https://arxiv.org/abs/1812.07252">RCAN paper</a> with different loss modeling (<a
            href="https://arxiv.org/abs/1603.08155">Perceptual/Feature Loss</a> instead of GAN loss).
        </p>
        <!-- LINKS
        –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://github.com/initmaks/ran2can">[github]</a>

      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="learning_to_swing" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/learning_to_swing.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Learning to swing</div>
        <br><em><span class="venue">2018</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          Computer Animation class project, which utilizes off-the-shelf Soft-Actor-Critic Reinforcement Learning method
          that learns to build up the momentum and swing the animated character on a pull up bar.
        </p>
        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="pages/charanim.html">[short-summary]</a>
      </div>
    </div>

    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="fetchit" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/fetchit.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">FetchIt</div>
        <br><em><span class="venue">2018</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          Mobile Manipulation project that utilises MoveIt! & GQ-CNN to grasp an object from the table using a Fetch
          Robot in the Gazebo Simulator.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="pages/fetchit.html">[short-summary]</a>
      </div>
    </div>


    <!-- PUBLICATION ENTRY –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row video-row">
      <!-- MEDIA SECTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
      <div class="four columns">
        <div class="video-container">
          <video id="car_bc" class="no-controls-video" autoplay loop muted preload="auto" playsinline>
            <source src="assets/videos/drive.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="eight columns text-column">
        <!-- TITLE –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <div class="project_title">Behavioral Clonning for Autonomous Driving</div>
        <br><em><span class="venue">2017</span></em>
        <!-- DESCRIPTION –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <p class="project_info">
          End-to-end (image-to-steering wheel) control policy learning from data collected over multiple laps with
          off-the-track recoveries generated by human.
        </p>

        <!-- LINKS –––––––––––––––––––––––––––––––––––––––––––––––––– -->
        <a href="https://github.com/initmaks/Self-driving_car_ND/tree/master/Behavioral-Cloning">[github]</a>
      </div>
    </div>

  </div>

  <br>
  <br>

  <!-- Teaching –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="div_line"></div><br><br>
    <div class="row profile-row">
      <div class="one columns hide-on-small">&nbsp;</div>
      <div class="nine columns ">
        <h5><u>Mentoring Experience</u></h5>
        I've had a great pleasure working with a number of exceptional students at Georgia Tech.<br><br>
        <ul>
          <li><strong>PRESENT</strong>: Master's Student - <a href="https://jxu443.github.io/portfolio/">Jiaxi Xu</a></li>
          <li><strong>FALL 2021</strong>: Master's Student - <a href="https://arjun-krishna.github.io/">Arjun Krishna</a> -> PhD student at UPenn’s GRASP lab </li>
          <li><strong>FALL 2020</strong>: Master's Student - <a href="https://qianluo.netlify.app">Qian Luo</a> ->
            NLP Algorithm Engineer at Alibaba Group</li>
        </ul>
        <br>
        <h5><u>Teaching Experience</u></h5>
        I had an amazing experience helping teach one of the largest classes (1000+ students) at Georgia Tech.
        <br>CS6601 - Artificial Intelligence class by
        <a href="https://www.cc.gatech.edu/people/thomas-ploetz"> Dr. Thomas Ploetz</a> & <a
          href="https://www.cc.gatech.edu/home/thad/">Dr. Thad Starner</a>.
        <br><br>
        <ul>
          <li><strong>FALL 2019 & SPRING 2020</strong>: Head Teaching Assistant </li>
          <li><strong>FALL 2018 & SPRING 2019</strong>: Teaching Assistant </li>
        </ul>
        <br>
        <h5><u>Scholarly Activities</u></h5>
        <ul>
          <li><strong>IROS 2023</strong> - Session co-chair Mechanism Design </li>
          <li><strong>RA-L 2023</strong> - Reviewer at IEEE Robotics and Automation Letters </li>
          <li><strong>RSS 2023</strong> - Reviewer at Proceedings of Robotics: Science and Systems </li>
          <li><strong>RA-L 2022</strong> - Reviewer at IEEE Robotics and Automation Letters</li>
          <li><strong>ICRA 2021</strong> - Reviewer at IEEE International Conference on Robotics and Automation</li>
        </ul>
      </div>
    </div>
  </div>


  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!-- attempt hiding from spambots :p - snippet credits Andrej Karpathy - karpathy.ai source code -->
  <script type="text/javascript">
    var e_is_shown = false;
    document.getElementById('iemail').addEventListener("click", function () {
      let demail = document.getElementById('demail');
      demail.innerHTML = 'm' + 'aks' + ' _a' + 't_ ' + 'gatech' + '.' + 'e' + 'du';
      demail.style.opacity = e_is_shown ? 0 : 1;
      e_is_shown = !e_is_shown;
    })
  </script>


  <!-- ### FOOTER ### -->

  <br>
  <br>

  <br>
  <br>
  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    <h7 style="text-decoration: none;">consider checking out: </h7><br><br>
    <a href="https://www.givewell.org/"><img src="assets/icons/gw.jpg" style="width:100px;"></a>
    <a href="https://www.givingwhatwecan.org/"><img src="assets/icons/gwwc.png" style="width:100px;"></a>
    <a href="https://www.effectivealtruism.org/"><img src="assets/icons/ea.png" style="width:80px; margin:10px"></a>

  </p>


  <div class="div_line"></div><br>

  <p style="color:gray; text-align:center; font-size: small;">
    2023© Maks Sorokin<br>
    built using <a href="http://getskeleton.com/">Skeleton</a>,
    icon credits <a href="https://www.flaticon.com/"> flaticon</a>,
    hosted by <a href="https://pages.github.com/"> GitHub Pages</a>❤️
    <br>
    <br>
    feel free to copy: <a href="https://github.com/initmaks/initmaks.github.io">this page</a>
  </p>

</body>

</html>